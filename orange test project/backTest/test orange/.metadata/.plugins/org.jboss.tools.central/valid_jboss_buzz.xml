<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Introduction to the Node.js reference architecture, Part 6: Choosing web frameworks</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks" /><author><name>Bethany Griggs</name></author><id>e2d2f76a-6caf-4300-871f-2bd3349ad843</id><updated>2021-12-03T07:00:00Z</updated><published>2021-12-03T07:00:00Z</published><summary type="html">&lt;p&gt;One of the key choices you make when building an enterprise &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application is the web framework that will serve as its foundation. As part of our &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview#"&gt;Node.js reference architecture effort&lt;/a&gt;, we've pulled together many internal Red Hat and IBM teams to discuss the web frameworks they've had success with. From our meetings, we've learned that most of the developers we spoke to are still happy with Express.js. This web framework has long been considered the default for Node.js, and it holds that place in our reference architecture as well.&lt;/p&gt; &lt;p&gt;However, Express.js is considered to be in maintenance mode. Thus, as part of the process of developing the reference architecture, we analyzed some data on web framework usage to try to get an idea of what might come next. In this article, you'll learn why Express.js is still a good fit for many Node.js developers and what the future could hold.&lt;/p&gt; &lt;p&gt;As with all our Node.js reference architecture recommendations, we focus on defining a set of good and reliable default choices. Some teams will deviate from some of these recommendations based on their assessment of what best fits their use case.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Read the series so far&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li class="Indent1"&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview" target="_blank"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 2: &lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/" target="_blank"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency" target="_blank"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;&lt;strong&gt;Part 6&lt;/strong&gt;: Choosing web frameworks&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Why Express.js?&lt;/h2&gt; &lt;p&gt;We consider Express.js a good default choice for a number of reasons:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;It's used widely, which means that there's a lot of shared knowledge about it both externally and within our organization.&lt;/li&gt; &lt;li&gt;New users can find a significant amount of resources to help them get started.&lt;/li&gt; &lt;li&gt;It has a relatively shallow dependency tree, with many dependencies maintained by the Express.js organization.&lt;/li&gt; &lt;li&gt;It's stable—it doesn't introduce breaking changes too frequently, but still addresses security vulnerabilities as necessary.&lt;/li&gt; &lt;li&gt;It's compatible across Node.js versions.&lt;/li&gt; &lt;li&gt;It's been used widely and successfully across IBM and Red Hat, including in the &lt;a href="https://developer.ibm.com/articles/evolving-the-ibm-cloud-console-with-microservices-a-nodejs-success-story/"&gt;IBM Cloud user interface&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;From our in-depth discussions as to which web framework we should recommend as our default choice, we also learned about and documented some other recommendations when using Express.js. Here are two key tips:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Register a liveness and readiness endpoint even if you're deploying initially to &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. These endpoints are useful in environments other than Kubernetes for problem determination and monitoring.&lt;/li&gt; &lt;li&gt;Use &lt;a href="https://www.npmjs.com/package/helmet"&gt;Helmet&lt;/a&gt; to set HTTP headers for a basic level of protection from some common attacks.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Learn more by reading the &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/functional-components/webframework"&gt;full details of our web framework recommendations&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Beyond Express.js: The next generation of web frameworks&lt;/h2&gt; &lt;p&gt;While Express.js is considered a good choice of web framework today, discussions and sentiment from our meetings indicate that might not continue to be the case in the future. Express.js is considered to be in maintenance mode, not in active development, and has not seen a new major release in more than five years. Because of this slow release cadence, there is concern that the framework might not keep up with the evolution of the Node.js runtime.&lt;/p&gt; &lt;p&gt;We came away from our discussion process suspecting that in the future our default web framework recommendation will change. As a result, we spent some time digging into various metrics to see what our recommended web framework might be five years from now.&lt;/p&gt; &lt;p&gt;Before we began this investigation, we needed to define its scope. We considered web frameworks that are likely to be used to handle requests and build APIs. We intentionally kept the initial pool of potential candidates as broad as possible, and tried to focus on use cases rather than looking for like-for-like frameworks.&lt;/p&gt; &lt;p&gt;For example, in the past, combining Node.js, Express.js, and a templating engine was a popular choice for building a web application. However, today you can solve the same problem using a dedicated static site framework. There are a lot more options in today's ecosystem, and where years ago for a given use case Express.js might have been the default choice, a more specialized framework might now exist for your use case.&lt;/p&gt; &lt;p&gt;We compiled an initial list of candidates from our reference architecture group discussions, as well as from lists of top Node.js frameworks compiled by outlets like &lt;a href="https://www.simform.com/blog/best-nodejs-frameworks/"&gt;Simform&lt;/a&gt; and &lt;a href="https://hackr.io/blog/nodejs-frameworks"&gt;Hackr.io&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Key metrics&lt;/h3&gt; &lt;p&gt;Once we had defined the candidates, we collated some key metrics for each of the frameworks, including:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Weekly downloads&lt;/li&gt; &lt;li&gt;npm dependents (that is, how many packages on npm depend on this module)&lt;/li&gt; &lt;li&gt;GitHub dependents&lt;/li&gt; &lt;li&gt;GitHub stars&lt;/li&gt; &lt;li&gt;Issues&lt;/li&gt; &lt;li&gt;Last release&lt;/li&gt; &lt;li&gt;Creation date&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Generally, the metrics across the board illustrated what we expected, as you can see in Figure 1. Express.js topped the download statistics, and saw the most dependents on both GitHub and npm.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Table of high-level metrics for each web framework" data-entity-type="file" data-entity-uuid="9a4374af-a2f7-4e1b-ab9a-7636e9a4a0a9" src="https://developers.redhat.com/sites/default/files/inline-images/high-level-metrics.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1. High-level metrics for each web framework.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;One other key takeaway is &lt;a href="https://nextjs.org/"&gt;Next.js&lt;/a&gt;'s relatively high position on the list, even though it's much newer than some of the other frameworks.&lt;/p&gt; &lt;h3&gt;Downloads&lt;/h3&gt; &lt;p&gt;Download metrics are not particularly useful for determining popularity, as the numbers can be heavily skewed by automation (from &lt;a href="https://developers.redhat.com/topics/ci-cd/all"&gt;continuous integration&lt;/a&gt; builds, for example), and also do not include organizations that use internal npm registries or caches.&lt;/p&gt; &lt;p&gt;However, these metrics can help make the relative positions of the frameworks clear. The graph in Figure 2, based on data collected on October 14, 2021, shows weekly npm downloads by web framework. Express.js dominates as expected, and Next.js is also in a strong position.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Diagram of weekly npm downloads by web framework" data-entity-type="file" data-entity-uuid="7547e627-c46e-4c7d-8d98-b21f1d3143e1" src="https://developers.redhat.com/sites/default/files/inline-images/npm-weekly-downloads.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2. Weekly npm downloads by web framework.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Another way of looking at npm download data is to examine the percentage share of registry downloads by module, as shown in Figure 3. This helps to account for the fact that overall registry downloads are increasing year over year. This information can be calculated using the npm API; for example, to get the total number of downloads for 2020, you can use the endpoint &lt;a href="https://api.npmjs.org/downloads/point/2021-01-01:2021-12-31"&gt;https://api.npmjs.org/downloads/point/2020-01-01:2020-12-31&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Diagram showing the percentage share of npm registry downloads by web framework" data-entity-type="file" data-entity-uuid="3e42a2aa-ef06-47a3-902d-1e7581f1b95b" src="https://developers.redhat.com/sites/default/files/inline-images/percent-share-registry-downloads.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3. Percentage share of npm registry downloads by web framework.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 3 shows that, over time, the number of downloads to the registry for Express.js as a proportion of the whole is declining. This doesn't necessarily indicate that Express.js usage is declining; the registry downloads might simply be becoming more spread out. We added React to our analysis as a comparative measure, and found that it's seeing a similar trend to Express.js.&lt;/p&gt; &lt;p&gt;Note that hapi is listed on the graph twice—the scoped and unscoped versions are treated separately.&lt;/p&gt; &lt;p&gt;In Figure 3, you can see that a number of less frequently downloaded frameworks are clumped together at the bottom of the chart. The trends here are interesting, so Figure 4 zooms in on them.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="A table of high-level metrics for each web framework (zoomed)" data-entity-type="file" data-entity-uuid="32177250-4ef5-4731-81be-a2df5383f1c3" src="https://developers.redhat.com/sites/default/files/inline-images/percent-share-registry-downloads-zoom.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4. Table of high-level metrics for each web framework (zoomed).&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Koa is going through a decline that's similar to what we saw with React and Express.js. Interestingly, we're seeing @hapi/hapi, Fastify, Nest.js, Next.js, and Nuxt.js all increasing, likely indicating that they're gaining popularity. However, the @hapi/hapi increase might be affected by the migration from hapi, the unscoped version of the module.&lt;/p&gt; &lt;h3&gt;Open Source Security Foundation criticality scores&lt;/h3&gt; &lt;p&gt;The Open Source Security Foundation (OpenSSF) has devised a &lt;a href="https://opensource.googleblog.com/2020/12/finding-critical-open-source-projects.html"&gt;criticality score&lt;/a&gt; that can be used to assess how critical a project is to the open source ecosystem as a whole. We generated criticality scores for all of our web framework candidates, with the results shown in Figure 5.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Open Source Security Foundation Criticality Scores" data-entity-type="file" data-entity-uuid="af91d7ba-5052-40a6-b9a0-df3bd9c5387a" src="https://developers.redhat.com/sites/default/files/inline-images/ossf-criticality.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 5. Open Source Security Foundation criticality scores.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next.js and Fastify generated the highest criticality scores, with Express.js further down the list. You can download the tool to generate these scores, along with documentation explaining more about how they're generated, from the &lt;a href="https://github.com/ossf/criticality_score"&gt;project's GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Contributions&lt;/h3&gt; &lt;p&gt;Contributions and contribution activity are also useful metrics for assessing frameworks. We began by looking at the contribution graphs generated by GitHub for each of the web frameworks we considered. (You can find this data by going to the Insights view on a project's GitHub repository.)&lt;/p&gt; &lt;p&gt;The top graph in Figure 6 illustrates a common scenario for many older web frameworks: An early peak in contribution activity, followed by a tailing off in more recent years. This could indicate that the project is in a maintenance phase rather than in active development. A few of the more recent frameworks, such as Next.js, demonstrated a more consistent pattern of activity, as illustrated in the lower graph in Figure 6.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Table of GitHub contribution activity graphs" data-entity-type="file" data-entity-uuid="604284bf-67e6-4a30-a0d7-3d163aa3bee5" src="https://developers.redhat.com/sites/default/files/inline-images/github-contribution-activity.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 6. GitHub contribution activity graphs.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Another metric that we considered was the distribution of contributions, so we looked at the share of commits per user (and their associated organizations). From this data, we could infer whether a project was mostly led by a specific individual, company, or community. Anton Whalley, Technology Partner Architect at IBM, created an application to generate these metrics using the GitHub API. You can &lt;a href="https://project-stats.netlify.app/"&gt;access the application here&lt;/a&gt; or &lt;a href="https://github.com/No9/project-stats"&gt;take a look at the source code&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As the charts in Figure 7 illustrate, some of the web frameworks we examined, like Framework 1, are mostly dominated by a single contributor. Others, like Framework 3, have a more spread out distribution of contributions.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Charts showing share of commits by GitHub user" data-entity-type="file" data-entity-uuid="e79e5656-f5dd-4f80-95fc-8ec6d135af30" src="https://developers.redhat.com/sites/default/files/inline-images/contributor-share.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 7. Share of commits per GitHub user.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The priorities of truly community-led frameworks are not controlled by a single entity, and new users are likely to be able to contribute on an equal level.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Our Node.js reference architecture recommendations are based on what teams across IBM and Red Hat have used successfully. As such, we expect that over time our recommendations will continue to evolve. We still feel that Express.js is a good default choice for a web framework today, though we're keeping in mind some of the known concerns about it.&lt;/p&gt; &lt;p&gt;At the same time, we're continuously evaluating other frameworks by looking at their metrics and considering the qualities that would favor a different candidate in the future. Ideally, a web framework should:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Be adequately maintained.&lt;/li&gt; &lt;li&gt;See regular releases.&lt;/li&gt; &lt;li&gt;Keep up-to-date with Node.js core features and changes.&lt;/li&gt; &lt;li&gt;Be community-led with open governance.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We'll continue to review internal and client usage and success stories. There are a few promising up-and-coming web framework candidates, but no single framework has outpaced the others enough to make it our new default recommendation—yet.&lt;/p&gt; &lt;p&gt;We hope you find these recommendations useful. While you wait for the next installment in the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Node.js reference architecture series&lt;/a&gt;, you can check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;GitHub project&lt;/a&gt; to explore sections that might be covered in future articles.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, you can also explore our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks" title="Introduction to the Node.js reference architecture, Part 6: Choosing web frameworks"&gt;Introduction to the Node.js reference architecture, Part 6: Choosing web frameworks&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bethany Griggs</dc:creator><dc:date>2021-12-03T07:00:00Z</dc:date></entry><entry><title type="html">RESTEasy Releases</title><link rel="alternate" href="https://resteasy.github.io/2021/12/02/resteasy-releases/" /><author><name /></author><id>https://resteasy.github.io/2021/12/02/resteasy-releases/</id><updated>2021-12-02T18:11:11Z</updated><dc:creator /></entry><entry><title>Anonymize data in real time with KEDA and Rook</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/02/anonymize-data-real-time-keda-and-rook" /><author><name>Huamin Chen, Yuval Lifshitz</name></author><id>ff2fa886-e78b-43c9-83f9-1cbf4bf9cb2b</id><updated>2021-12-02T07:00:00Z</updated><published>2021-12-02T07:00:00Z</published><summary type="html">&lt;p&gt;Data privacy and data protection have become increasingly important globally. More and more jurisdictions have passed data privacy protection laws to regulate operators that process, transfer, and store data. &lt;a href="https://en.wikipedia.org/wiki/Pseudonymization"&gt;Data pseudonymization&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Data_anonymization"&gt;anonymization&lt;/a&gt; are two common practices that the IT industry turns to in order to comply with such laws.&lt;/p&gt; &lt;p&gt;In this article, you'll learn about an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; cloud-native solution architecture we developed that allows managed data service providers to anonymize data automatically and in real time.&lt;/p&gt; &lt;h2&gt;Data anonymization&lt;/h2&gt; &lt;p&gt;Pseudonymized data can be restored to its original state through a different process, whereas anonymized data cannot. Encryption is a typical way to pseudonymize data; for example, encrypted data can be restored to its original state through decryption. On the other hand, anonymization is a process that completely removes sensitive and personal information from data. In Figures 1 and 2, images containing license plates and faces are anonymized via blurring to remove sensitive and personal information.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/fig1.jpg?itok=3_LvR6qU" width="496" height="142" alt="Face anonymization." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1. Face anonymization.&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2%20%281%29.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/fig2%20%281%29.jpg?itok=9QYhWeDb" width="684" height="142" alt="License plate anonymization." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2. License plate anonymization.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once data has been anonymized, its use is no longer subject to the &lt;a href="https://en.wikipedia.org/wiki/Data_anonymization#GDPR_requirements"&gt;strict requirements of GDPR&lt;/a&gt;, the European Union's data protection law. In many use cases, anonymizing data facilitates that data's public exchange.&lt;/p&gt; &lt;h2&gt;Solution architecture overview&lt;/h2&gt; &lt;p&gt;As illustrated in Figure 3, this solution uses Cloud Native Computing Foundation (CNCF) projects such as &lt;a href="https://www.cncf.io/projects/rook/"&gt;Rook&lt;/a&gt; (serving as the infrastructure operator) and &lt;a href="https://www.cncf.io/projects/keda/"&gt;KEDA&lt;/a&gt; (providing a &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless&lt;/a&gt; framework), along with &lt;a href="https://www.rabbitmq.com/"&gt;RabbitMQ&lt;/a&gt; for its message queue, and &lt;a href="https://pjreddie.com/darknet/yolo/"&gt;YOLO&lt;/a&gt; and &lt;a href="https://opencv.org/"&gt;OpenCV&lt;/a&gt; for object detection without loss of generality. The containerized workloads, Rook, RabbitMQ, or KEDA, run on &lt;a href="https://microshift.io/"&gt;MicroShift&lt;/a&gt;, a small footprint &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;/&lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift&lt;/a&gt; implementation.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Copy%20of%20anonymization-arch-2.jpeg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Copy%20of%20anonymization-arch-2.jpeg?itok=AIGyWMUo" width="1440" height="671" alt="Diagram of the solution architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3. Solution architecture.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Under this architecture, a &lt;a href="https://www.redhat.com/en/technologies/storage/ceph"&gt;Ceph&lt;/a&gt; Object Gateway—also known as RADOS Gateway (RGW) bucket—provisioned by Rook, is configured as a &lt;a href="https://docs.ceph.com/en/latest/radosgw/notifications/"&gt;bucket notification&lt;/a&gt; endpoint to a RabbitMQ exchange. A KEDA RabbitMQ trigger is created to probe the queue lengths in the exchange. Once the queue length exceeds the threshold, a serverless function, implemented as a &lt;code&gt;StatefulSet&lt;/code&gt;, scales up, reads queue messages, parses bucket and object information, retrieves the object, detects regions of interest, blurs sensitive information, and replaces the original data with transformed data.&lt;/p&gt; &lt;h2&gt;Building the solution&lt;/h2&gt; &lt;p&gt;Bringing all these CNCF projects into one cohesive solution sounds complex. But it isn't in practice, thanks to the declarative, YAML-based configuration used by the different operators that make up the solution. Full instructions, scripts, and a recorded demo are &lt;a href="https://github.com/redhat-et/Real-Time-Data-Anonymization"&gt;available here&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;MicroShift&lt;/h3&gt; &lt;p&gt;First, we need to install &lt;a href="https://github.com/redhat-et/microshift"&gt;MicroShift&lt;/a&gt;. The solution would work in a fully fledged OpenShift or Kubernetes cluster, but if you want to get it up and running quickly on your laptop, then MicroShift is your most lightweight option.&lt;/p&gt; &lt;p&gt;First, create a default &lt;code&gt;StorageClass&lt;/code&gt; and use &lt;code&gt;hostpath-provisioner&lt;/code&gt; in the MicroShift cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: hostpath-provisioner spec: capacity: storage: 8Gi accessModes: - ReadWriteOnce hostPath: path: "/var/hpvolumes" EOF kubectl patch storageclass kubevirt-hostpath-provisioner -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Another option would be to run the solution on &lt;a href="https://minikube.sigs.k8s.io/docs/"&gt;minikube&lt;/a&gt;. But minikube uses a virtual machine (VM), so you would need to add an extra disk to the minikube VM before installing Rook.&lt;/p&gt; &lt;h3&gt;Rook&lt;/h3&gt; &lt;p&gt;Next, install the Rook operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/crds.yaml kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/common.yaml kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/operator-openshift.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that you have to use a developer image in &lt;code&gt;operator-openshift.yaml&lt;/code&gt;, because bucket notifications in Rook are still a work in progress.&lt;/p&gt; &lt;p&gt;Before moving on, verify that the Rook operator is up and running:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl get pods -l app=rook-ceph-operator -n rook-ceph&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to create a Ceph cluster. Use &lt;code&gt;cluster-test.yaml&lt;/code&gt;, because you'll be running the solution on a single node.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/cluster-test.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that you'll use a custom Ceph image in &lt;code&gt;cluster-test.yaml&lt;/code&gt; to work around the RabbitMQ plaintext password limitation.&lt;/p&gt; &lt;p&gt;Next, verify that the cluster is up and running with monitors and OSDs:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl get pods -l app=rook-ceph-mon -n rook-ceph kubectl get pods -l app=rook-ceph-osd -n rook-ceph &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, you should add the object store front end to the Ceph cluster, together with a toolbox that allows you to run administrative commands:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/object-test.yaml kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/toolbox.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can now verify that the RGW is up and running:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl get pods -l app=rook-ceph-osd -n rook-ceph &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At this point, everything is set up to create the bucket with its &lt;code&gt;StorageClass&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat &lt;&lt; EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rook-ceph-delete-bucket-my-store provisioner: rook-ceph.ceph.rook.io/bucket # driver:namespace:cluster reclaimPolicy: Delete parameters: objectStoreName: my-store objectStoreNamespace: rook-ceph # namespace:cluster region: us-east-1 --- apiVersion: objectbucket.io/v1alpha1 kind: ObjectBucketClaim metadata: name: ceph-delete-bucket labels: bucket-notification-my-notification: my-notification spec: bucketName: notification-demo-bucket storageClassName: rook-ceph-delete-bucket-my-store EOF &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the bucket has a special label called &lt;code&gt;bucket-notification-my-notification&lt;/code&gt; with the value &lt;code&gt;my-notification&lt;/code&gt;. This indicates that notifications should be created for this bucket. The notification custom resource hasn't been created yet, but the notification will be defined for the bucket once it has been. That's the beauty of declarative programming.&lt;/p&gt; &lt;p&gt;To create the topic and notification, you'll need some details from the RabbitMQ broker, so you'll configure that next.&lt;/p&gt; &lt;h3&gt;RabbitMQ&lt;/h3&gt; &lt;p&gt;Begin by installing the RabbitMQ operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, define a RabbitMQ cluster. You can use the default "hello world" cluster for the purposes of this article:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/rabbitmq/cluster-operator/main/docs/examples/hello-world/rabbitmq.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There's one manual step you'll need to take to sync up definitions between RabbitMQ and the bucket notifications topic you defined in Rook. First, define the topic exchange in RabbitMQ with the same name that you used to define in the bucket notifications topic.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl exec -ti hello-world-server-0 -c rabbitmq -- rabbitmqadmin declare exchange name=ex1 type=topic&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, get the username/password and the service name defined for the RabbitMQ cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;username="$(kubectl get secret hello-world-default-user -o jsonpath='{.data.username}' | base64 --decode)" password="$(kubectl get secret hello-world-default-user -o jsonpath='{.data.password}' | base64 --decode)" service="$(kubectl get service hello-world -o jsonpath='{.spec.clusterIP}')" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You'll use these to create the bucket notification topic, pointing to that RabbitMQ cluster.&lt;/p&gt; &lt;p&gt;You are about to send a username/password as part of the configuration of the topic, and in production, that would require a secure connection. For the sake of this demo, however, you can use a small workaround that allows you to send them as cleartext:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph config set client.rgw.my.store.a rgw_allow_secrets&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can configure the bucket notification topic:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat &lt;&lt; EOF | kubectl apply -f - apiVersion: ceph.rook.io/v1 kind: CephBucketTopic metadata: name: demo spec: endpoint: amqp://${username}:${password}@${service}:5672 objectStoreName: my-store objectStoreNamespace: rook-ceph amqp: ackLevel: broker exchange: ex1 EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also configure the notification configuration (the same one from the special &lt;code&gt;ObjectbucketClaim&lt;/code&gt; label) pointing at that topic:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat &lt;&lt; EOF | kubectl apply -f - apiVersion: ceph.rook.io/v1 kind: CephBucketNotification metadata: name: my-notification spec: topic: demo filter: events: - s3:ObjectCreated:* EOF&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;KEDA&lt;/h3&gt; &lt;p&gt;The last component in the configuration is KEDA, a very generic framework that allows autoscaling based on events. In this example, you will use it as a serverless framework that scales its functions based on the RabbitMQ queue length.&lt;/p&gt; &lt;p&gt;The installation of KEDA is based on &lt;a href="https://developers.redhat.com/topics/helm/all"&gt;Helm&lt;/a&gt;, so you should &lt;a href="https://helm.sh/docs/intro/install/"&gt;download&lt;/a&gt; and then install Helm if you haven't done so already. Once you have, use it to install KEDA.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;helm repo add kedacore https://kedacore.github.io/charts helm repo update kubectl create namespace keda helm install keda kedacore/keda --version 1.4.2 --namespace keda&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to tell KEDA what to do. The KEDA framework will be polling the queue from the RabbitMQ cluster you configured in the previous steps. That queue does not exist yet, so you need to create it first:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl exec -ti hello-world-server-0 -c rabbitmq -- rabbitmqadmin declare queue name=bucket-notification-queue durable=false kubectl exec -ti hello-world-server-0 -c rabbitmq -- rabbitmqadmin declare binding source=ex1 destination_type=queue destination=bucket-notification-queue routing_key=demo&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the name of the routing key (&lt;code&gt;demo&lt;/code&gt;) must match the name of the bucket notification topic.&lt;/p&gt; &lt;p&gt;KEDA will also need to know the RabbitMQ username/password and service, and the function it spawns will need to know the credentials of the Ceph user so it can read the objects from the bucket and write them back after anonymizing. You already retrieved the RabbitMQ secrets in the previous step, so now you need to get the Ceph ones:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;user=$(kubectl -n rook-ceph -it deploy/rook-ceph-tools -- radosgw-admin user list | grep ceph-user |cut -d '"' -f2) aws_key_id=$(kubectl -n rook-ceph -it deploy/rook-ceph-tools -- radosgw-admin user info --uid $user | jq -r '.keys[0].access_key') aws_key=$(kubectl -n rook-ceph -it deploy/rook-ceph-tools -- radosgw-admin user info --uid $user | jq -r '.keys[0].secret_key') aws_url=$(echo -n "http://"$(kubectl get service -n rook-ceph rook-ceph-rgw-my-store -o jsonpath='{.spec.clusterIP}') |base64) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can configure the secrets for KEDA and the RabbitMQ scaler for KEDA:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: rabbitmq-consumer-secret data: amqp_url: amqp://${username}:${password}@${service}:5672 --- apiVersion: v1 kind: Secret metadata: name: rgw-s3-credential data: aws_access_key: ${aws_key} aws_key_id: ${aws_key_id} aws_endpoint_url: ${aws_url} --- apiVersion: keda.k8s.io/v1alpha1 kind: ScaledObject metadata: name: rabbitmq-consumer namespace: default spec: scaleTargetRef: deploymentName: rabbitmq-consumer triggers: - type: rabbitmq metadata: host: amqp://${username}:${password}@${service}:5672 queueName: "bucket-notification-queue" queueLength: "5" authenticationRef: name: rabbitmq-consumer-trigger --- apiVersion: keda.k8s.io/v1alpha1 kind: TriggerAuthentication metadata: name: rabbitmq-consumer-trigger namespace: default spec: secretTargetRef: - parameter: host name: rabbitmq-consumer-secret key: amqp_url EOF &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Anonymizing data&lt;/h2&gt; &lt;p&gt;Finally, you're ready to actually anonymize data. This example uses the &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/install-linux.html"&gt;awscli&lt;/a&gt; tool, but you could use any other tool or application that can upload objects to Ceph.&lt;/p&gt; &lt;p&gt;awscli uses environment variables to get the user credentials needed; you can use the ones fetched in previous steps:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;export AWS_ACCESS_KEY_ID=$aws_key export AWS_SECRET_ACCESS_KEY=$aws_key_id&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, you need to upload the image you want to anonymize:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;aws --endpoint-url $aws_url s3 cp image.jpg s3://notification-demo-bucket/&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The KEDA logs will show the serverless function scaling up and down:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl logs -n keda -l app=keda-operator -f&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the serverless function's logs will show how it fetches the images, blurs them, and writes them back to Ceph:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl logs -l app=rabbitmq-consumer -f&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The RADOS Gateway logs will show the upload of the image, the sending of the notification to RabbitMQ, and the serverless function fetching the image and then uploading the modified version of it:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl logs -l app=rook-ceph-rgw -n rook-ceph -f &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This open source solution, based on CNCF projects, addresses pressing needs for data privacy protection. We believe this solution can be extended to other use cases, such as malware and ransomware detection and quarantine and data pipeline automation. For instance, ransomware encrypts objects without authorization. Our solution can help detect ransomware activities by running a serverless function to detect entropy changes as soon as an object is updated or created. Similarly, a serverless function can run, scan, and quarantine newly created objects if viruses or malware are detected.&lt;/p&gt; &lt;p&gt;If you're excited about the project or have new ideas, please share your success stories at &lt;a href="https://github.com/redhat-et/Real-Time-Data-Anonymization"&gt;our GitHub repository&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/02/anonymize-data-real-time-keda-and-rook" title="Anonymize data in real time with KEDA and Rook"&gt;Anonymize data in real time with KEDA and Rook&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Huamin Chen, Yuval Lifshitz</dc:creator><dc:date>2021-12-02T07:00:00Z</dc:date></entry><entry><title>Automating Quarkus with GitLab</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/logicdrop-automating-quarkus-with-gitlab/&#xA;            " /><author><name>KimJohn Quinn (https://twitter.com/logicdrop)</name></author><id>https://quarkus.io/blog/logicdrop-automating-quarkus-with-gitlab/</id><updated>2021-12-02T00:00:00Z</updated><published>2021-12-02T00:00:00Z</published><summary type="html">Logicdrop provides a business automation and data intelligence platform that enables enterprises to design their own solutions and run them in the cloud. We process millions of transactions a month across a global client-base and add significant value to mission-critical processes. The Challenge Since moving to Quarkus, our productivity quickly...</summary><dc:creator>KimJohn Quinn (https://twitter.com/logicdrop)</dc:creator><dc:date>2021-12-02T00:00:00Z</dc:date></entry><entry><title type="html">Implementing the Filter EIP using Camel-k and Kogito</title><link rel="alternate" href="https://blog.kie.org/2021/12/implementing-the-filter-eip-using-camel-k-and-kogito.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/12/implementing-the-filter-eip-using-camel-k-and-kogito.html</id><updated>2021-12-01T22:30:15Z</updated><content type="html">Apache Camel is an Open Source integration framework that supports the implementation of the Enterprise Integration patterns (EIP). The Message Filter EIP allows you to eliminate undesired messages from a channel based on a set of criteria. The pattern uses a predicate to decide if the messages should be dropped or not.  Oftentimes the predicate logic can be complex, and might need more transparency and control by the business user. For instance, let us say we have transactions coming in, and we are only interested in the high risk transactions. As you can imagine, the rules that govern the risk can be complicated. This is where a rules engine can come in handy, to provide for a business user friendly methodology to define the predicate logic.  In this article we will discuss how to implement the Filter EIP using the cloud native technologies – Kogito and Camel-K. Kogito is designed to deliver powerful capabilities for building process and rules based applications natively in the cloud on a modern container platform. Camel K is a lightweight integration framework built from Apache Camel to run natively on Kubernetes DEFINING THE TRANSACTION RISK RULES Let us first define the transaction risk rules. As you can see below, the transaction risk is based on transactionAmount, transactionCountry and merchant information. DEPLOYING THE DECISION SERVICE The kogito project for this decision can be found . Let us now deploy this DMN as a kogito decision service on openshift. For this install the kogito operator. Now let us create a kogito build using the repo above. After the Build and Deploy the kogito decision service exposes a route so that we can invoke the decision service. We can now invoke the decision service and test it. DEFINING THE CAMEL ROUTE Let us now define the camel route. We first invoke the decision service. The service returns back a payload with a boolean value indicating the risk. We then use this decision result in the predicate for deciding if this transaction needs further processing. As you can see, the complexity of deciding if the transaction is high risk is offloaded to the kogito decision layer. The entire camel route definition can be found . DEPLOYING THE CAMEL ROUTE USING CAMEL-K We will now install the Camel K operator and deploy the integration micro service. kamel install Kamel run FilterEIP.java TESTING THE INTEGRATION MICROSERVICE Now we can lookup the route that is auto generated by the REST DSL component and send in a transaction payload. We can now see that the data is now available in the kafka topic. Notice the header information, where we set the decision evaluation to decide if the transaction needs to be sent to kafka. We will now send in a request, with a low risk. Now, we can see the risk is evaluated to be false and this transaction is filtered for post processing, so we don’t see it on the kafka topic. Similarly, it is possible to plug in decision points to other EIP patterns like content routing. Check out this from Matteo Mortari about how you can use DMN decisions for content based routing in Camel. References The post appeared first on .</content><dc:creator>Sadhana Nandakumar</dc:creator></entry><entry><title>How DevSecOps brings security into the development process</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/01/how-devsecops-brings-security-development-process" /><author><name>Andy Oram</name></author><id>d9d5c475-8943-4b44-a5b9-d478a4eff697</id><updated>2021-12-01T07:00:00Z</updated><published>2021-12-01T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/devsecops"&gt;DevSecOps&lt;/a&gt; is an extension of &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; that emphasizes security automation and cooperation across the organization. More than just hype, DevSecOps is a crucial addition to your organization's development and deployment processes, especially given the range of ransomware groups, industrial spies, identity thieves, and other attackers plaguing today's cyberworld. In this article, you will learn how DevSecOps extends familiar DevOps tools and processes to help cross-functional teams work together on the design and implementation of &lt;a href="https://developers.redhat.com/topics/security"&gt;security&lt;/a&gt; policies and procedures.&lt;/p&gt; &lt;h2&gt;What is DevSecOps?&lt;/h2&gt; &lt;p&gt;Essentially, DevSecOps is a way to ensure that security policies set by your organization—such as static analysis, vulnerability scanning, and access controls—are applied consistently in production, even if you are launching hundreds of virtual machines or containers every hour. With DevSecOps, the tools that carry out security policies are baked into the build process, through well-known DevOps techniques such as &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration and continuous deployment&lt;/a&gt; (CI/CD). The development team can assure managers and administrators that their security policies are being enforced without depending on individual developers to manually run the tools.&lt;/p&gt; &lt;h2&gt;DevSecOps tools and processes&lt;/h2&gt; &lt;p&gt;Developers and teams familiar with DevOps tools and processes can adopt them for DevSecOps. The basic elements of DevSecOps are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: DevSecOps adds vulnerability scanners, penetration testing, firewall rules, intrusion detection systems, and other common security features to the version control and CI/CD processes used for DevOps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Processes&lt;/strong&gt;: DevSecOps automates security practices in order to apply them consistently and verifiably across all the containers and services created by the development team.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparency and review&lt;/strong&gt;: All decisions made by the development team are open to discussion among managers and security experts. Test and production systems can log their activities, and these logs can be checked to ensure that the development team has implemented the decisions of the larger organization.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Let's look at a few hypothetical examples of DevSecOps in action to show how it brings organizational priorities into production.&lt;/p&gt; &lt;h2&gt;Vulnerability scanning in the DevSecOps pipeline&lt;/h2&gt; &lt;p&gt;Vulnerabilities exist at many levels. You may have coding errors such as buffer overflows and incorrect type conversions, poorly secured user interfaces that allow SQL injection, or dependencies on third-party libraries that contain security flaws. A range of automatic vulnerability checkers now exists for all these problems, suited to various application types and sizes. Many of these tools can be added to a build process with just a few clicks in popular developer repositories, such as &lt;a href="https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/devsecops-in-github"&gt;DevSecOps in GitHub&lt;/a&gt; and &lt;a href="https://about.gitlab.com/solutions/dev-sec-ops/"&gt;DevSecOps with GitLab&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;But a developer can't check everything all the time. The team must decide at what point to run checks, how much time they want to add to the build process, and where the most urgent priorities lie. A DevSecOps pipeline documents these decisions and ensures they are carried out.&lt;/p&gt; &lt;p&gt;For instance, where should you incorporate a tool such as Red Hat's &lt;a href="https://developers.redhat.com/articles/2021/11/25/build-and-extend-containerized-applications-project-thoth"&gt;Project Thoth&lt;/a&gt;, which checks common security databases and reveals flaws in your third-party libraries? If you discover a problem in a function call buried deep within your application, you had better determine right away whether your application is at risk, and if so how to fix the problem. Should you upgrade, back off to an old version of the library, or replace the library altogether? You'll want to learn about the flaw as early in the development cycle as you can, so you might take the time to run the tool upon every check-in to version control.&lt;/p&gt; &lt;p&gt;On the other hand, when it comes to running a common code scanner for, say, bugs in memory management code, you might choose to wait until you are ready to build a full version of the app for testing. Memory management flaws might be critical, but they are usually quick to fix.&lt;/p&gt; &lt;p&gt;The key to DevSecOps is that a team and its security advisors can discuss the tools and trade-offs available for each code base and then bake their decisions into builds.&lt;/p&gt; &lt;h2&gt;Dynamic scanning and penetration testing&lt;/h2&gt; &lt;p&gt;Security experts know that they can't rely on applications to be safe in a production environment, even if the developers have run a battery of static tests. DevSecOps lets developers ensure that every container they launch is checked regularly at runtime by a penetration tester, intrusion detection system, and other such tools.&lt;/p&gt; &lt;h2&gt;An automated approach to securing devices&lt;/h2&gt; &lt;p&gt;Many organizations are losing control over their endpoints. During the COVID-19 shutdowns, workers obtained access to critical systems and assets from their homes and even their local cafes. Within the workplace, "bring your own device" (BYOD) became popular well before that.&lt;/p&gt; &lt;p&gt;The standard security practice in this situation is to scan networks for all devices that connect. A database of approved devices helps restrict access to authorized devices. Such access can also be restricted to particular locations and times of the week. Penetration tests and other tools can even check whether the device has a password and up-to-date software.&lt;/p&gt; &lt;p&gt;To institute such protections, the IT team has to consult with managers of all departments. The organization needs processes for registering devices along with their owners, and for scanning networks regularly. Some of these processes can be incorporated into the development process through DevSecOps. For instance, your configurations can ensure that a scanner is running, as well as a process that monitors logs and alerts.&lt;/p&gt; &lt;p&gt;Similar processes can formalize other organizational protections, such as firewall rules and access control lists.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;DevSecOps is about more than tools and processes. It brings transparency and validation to the crucial area of cybersecurity. All relevant stakeholders can weigh in on security decisions and be sure that the developers incorporate their concerns into development and build processes. Security on a 24/7 basis no longer depends on the day-to-day vigilance of developers or operators; instead, it is enforced by a system where cross-functional teams are working in alignment.&lt;/p&gt; &lt;p&gt;In organizations with constant innovation and heavy dependence on network applications interacting with people around the world, DevSecOps gives developers, operators, and managers more peace of mind.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/01/how-devsecops-brings-security-development-process" title="How DevSecOps brings security into the development process"&gt;How DevSecOps brings security into the development process&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andy Oram</dc:creator><dc:date>2021-12-01T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.5.1.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-5-1-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-5-1-final-released/</id><updated>2021-12-01T00:00:00Z</updated><published>2021-12-01T00:00:00Z</published><summary type="html">Today, we released Quarkus 2.5.1.Final, a maintenance release for our 2.5 release train containing bugfixes and documentation improvements. It is a safe upgrade for anyone already using 2.5. If you are not using 2.5 already, please refer to the 2.5 migration guide. Full changelog You can get the full changelog...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2021-12-01T00:00:00Z</dc:date></entry><entry><title type="html">TrustyAI SHAP: Overview and Examples</title><link rel="alternate" href="https://blog.kie.org/2021/11/trustyai-shap-overview-and-examples.html" /><author><name>Rob Geada</name></author><id>https://blog.kie.org/2021/11/trustyai-shap-overview-and-examples.html</id><updated>2021-11-30T09:17:42Z</updated><content type="html">SHAP is soon to be the newest addition to the TrustyAI explanation suite. To properly introduce it, let’s briefly explore what SHAP is, why it’s useful, and go over some tips about how to get the best performance out it. A BRIEF OVERVIEW SHAPLEY VALUES The core idea of a SHAP explanation is that of a Shapley value, a concept from game theory that won its eponymous inventor Lloyd Shapley the 2012 Nobel Prize in Economics. Shapley values are designed to solve the following problem: &gt; Given some coalition C of members that cooperatively produce some value V, how &gt; do we fairly determine the contribution p of each member in C?  Here, a coalition simply refers to a group of cooperating members that work together to produce some value V, called the coalition value. This could be something like corporation of employees that together generate a certain profit, or a dinner group running up a restaurant bill. We want to know exactly how much each member contributed to that final coalition value, what share of the profit each employee deserves, how much each person in the dinner party owes to settle the bill. In simple cases, this can be easy enough to manually figure out, but it can get tricky when there are interacting effects between members, when certain permutations cause members to contribute more or less than the sum of their parts.  A SIMPLE EXAMPLE As an example, let’s look at a coalition that contains 4 members: Red, Orange, Yellow, and Green. Let’s imagine that these are tokens in a game, where your score is calculated as follows: 1. Having the red token adds 5 points to your score. Yellow adds 4 points, while orange and green both add 3. 2. Having any two colors grants a 5 point bonus. 3. Having any three colors grants a 7 point bonus. 4. Having all four colors grants a 10 point bonus. Under these rules, having all four tokens would result in a score of 25: first, we add the raw token values 5+4+3+3=15, then add 10 points from the 4-token bonus. Now the question is, how many of those 25 points were contributed by the red token? This is exactly what Shapley values seek to answer, and do this via a property called average marginal contribution. To compute this, we first look at every possible subcoalition of the members of our coalition, that is, listing all of the different ways of selecting anywhere between 0 and 4 tokens: Then, we organize these coalitions into pairs, such that each member in the pair differ only by the inclusion of the particular member we’re investigating. In this case, we’ll pair coalitions that differ only by the inclusion of the red token: We then find the coalition value of each of these subcombinations, that is, determine the score that each particular coalition of tokens would produce. Then, we find the difference between the two paired coalitions: the difference in score caused by adding the red token: The difference in scores within each pair is called the marginal contribution. Each marginal contribution is then weighted according to how common that particular comparison is. In this example, there are three pairs that compare a 2-token coalition to a 1-token coalition, meaning their weights are each 1/3. Meanwhile, there is only one pair that compares a 1-token coalition to a 0-token coaltion, and thus its weight is 1/1.  We then find the weighted sum of each marginal contribution multiplied by its corresponding weight: Dividing this number by the total number of members in our original coalition (4) gives us the red token’s Shapley value. This is a measure of how much the addition of a red token adds on average to any arbitrary grouping of tokens. In our case, the red token’s Shapley value is 30 ÷ 4 = 7.5, which means that in our original four token hand, the red token contributed 7.5 of our 25 points. We can repeat this process for the other tokens, and now we’ve computed the Shapley values for all four tokens: Therefore, of the 25 points our hand was worth, the red token contributed 7.5 points, the yellow 6.5, and the orange and green 5.5. This seems like very fair way of assigning credit for the total score: each Shapley value is equal to the token’s base score, plus 1/4 of the 10-point four-color bonus.   SHAPLEY VALUES AND EXPLAINABILITY With Shapley values, we now have a means of fairly describing the influence any certain member had on the value of the coalition. We can adapt this concept relatively easily to produce model explanations by instead looking to describe the influence any certain input feature had on the model output. Here, an input feature simply refers to a discrete component of a model’s input; for example, if the model processes loan applications, the input features might be things like the applicant’s name or age. While this seems straightforward enough, computing the Shapley values of an n-feature model in practice requires passing 2n different feature permutations through a model. In situations where we have lots of features or expensive model evaluation, evaluating all 2n combinations may not be feasible. For example, if we had a 25 feature model that takes just one second to evaluate, evaluating all 225 permutations would take over a year! Furthermore, there is an additional practical obstacle: you cannot simply omit a feature from the input of most decision models without having to either retrain or redesign the entire model. SHAP To rectify these problems, Scott Lundberg and Su-In Lee devised the Shapley Kernel in a titled “A Unified Approach to Interpreting Model Predictions”. COMBINATION ENUMERATION To solve the problem of needing to enumerate every possible feature permutation, the Shapley Kernel instead evaluates just a small sample of combinations. Each of the combinations is then assigned some binary vector, where a 1 in the ith position of the vector indicates that the ith feature is included, while 0 indicates exclusion. This gives us a sample of vectors and coalition values, from which we can write a simple system of linear equations. From this system, we can fit a to find A, B, C, and D. With a very specific choice of weightings calculated from various combinametric statistics, we can actually ensure that the weighted linear regression recovers the Shapley values of their respective features! This is the key insight of the Shapley Kernel, that a simple regression over a sample of feature permutations can provide an excellent estimation of Shapley values. With these Shapley values, we can approximate any model with an explanatory model of the form: This gives a very easy to understand, additive explanation of each feature’s contribution to the model output; we can simply look at each Shapley value to know the effect each feature had. BACKGROUND DATA Next to solve is the feature omission problem. Lundberg and Lee do this by simpling reframing what it means to omit a feature. Rather then omit a feature entirely from the model input, they instead replace it with a variety of its typical values, called the background (noise) of the feature. This background feature values are taken from a background dataset, a set of around 100 or so typical inputs to the model. Instead of computing the marginal contribution as the difference between the model output with the feature included versus excluded, we instead find the difference between the model output with the desired feature value versus a background value of that feature. We do this for every background feature value from the background dataset, and take the average of all of those comparisons as our marginal contribution:  Under this reformulation, the Shapley value is measuring how much this particular value of a feature contributed to the model’s output, as compared to background values of the feature.  This introduces a very important concept to SHAP, and that is the background value of the model: the average model output over all the background datapoints. Mathematically, this serves to add an intercept to our explanatory model: Therefore, our model output is equal to the sum of all Shapley values, plus the model background value.  A “REAL WORLD” EXAMPLE Let’s take a look at using SHAP to explain a real world prediction. If you’d like to follow along, the code for this example is found , and the code for generating the plots is . Let’s imagine we’re a bank, and we automatically determine loan eligibility via a . Our model receives seven features as input, the applicant age, their annual income, their number of children, their number of consecutive days of employment, and whether or not they own property, a work phone, or car. From this information, our model outputs a number between 0% and 100%; the applicant’s probability of being accepted for a loan. Let’s say we’ve just received an applicant with the following details: From this input, our model outputs that our applicant has a 65.9% chance of approval. However, if we want to understand why that particular decision was made, we need to be able to provide an explanation of how the applican’ts features affected the output and thus we turn to SHAP. To run SHAP, we need a set of background datapoints from which we can generate our background feature values. In this example, I just randomly generate 100 background datapoints, but when using SHAP for real you might select data points from the model training data or in such a way that the background has desirable properties. In our case, the model output over our 100 randomly-generated background datapoints gave an average approval probability of 46.60%, which thus sets our background value. To better interpret this background, let’s take a look at the average feature value from our background data as compared to our applicant: Here we see that our applicant has a higher-than-background age, income, number of children, and days employed. Like a majority of background dataset applicants, they own realty and do not own a workphone, but are in the minority that own a car. With an idea of how our applicant differs from the typical background applicant, we can now run SHAP to see how these differences affected the model output.  First, we configure SHAP to use these background datapoints, then pass it our random forest classifier and our applicant’s feature data. After SHAP finishes running, we are returned a list of Shapley values, which we then visualize in a candlestick chart: In this chart blue bars indicate that our applicant’s feature difference versus the background has a positive effect on their approval rate, while red bars indicate the difference negatively affected their approval. Therefore, the most positive impact was our applicant’s higher-than-background income, which increased their probability of acceptance by 11.10 percentage points. Meanwhile, their higher-than-typical number of chilren was the most negative impact, reducing their acceptance probability by 1.97 percentage points. Both of these make a lot of intuitive sense; a larger income would naturally increase your ability to pay back a loan, whereas having more children and thus more financial dependents might decrease that ability.  These results indicate that both our random forest model’s decision-making process and our explanatory model’s explanations align with our domain intuition, which is an excellent indicator of the quality of both. This is one of the great advantages of explainability; not only does it provide transparency into model decisions, it also allows for sanity-checking of model behavior.  TIPS AND TRICKS INTERPRETING SHAP EXPLANATIONS As we’ve seen, a SHAP value describes the effect a particular feature had on the model output, as compared to the background features. This comparison can introduce some confusion as to the meaning of the raw Shapley values, and make finding clear intuition a little trickier. This is especially true when the reference points are not clearly defined.  For example: * “Your income increased your acceptance probability by 11.10 percentage points.” * “You own a car, which reduced your acceptance probability 0.67 percentage points.” This explains the income’s effect on the outcome, but does not answer what about the applicant’s income affected the result. Was it too high? Was it too low? There is an explanation, but it is too vague to be actionable in any meaningful way. Instead, we can try directly comparing the applicant’s feature value to the average background value of that feature. This makes the Shapley value’s implicit comparison to the background explicitly clear: * “Your income is $36k higher than a typical income, which increased your acceptance probability by 11.10 percentage points.” * “You own a car, which we see in only 39% of typical loan applicants. Because of this, your acceptance probability was reduced by 0.67 percentage points.” Not only does this convey the comparison being made by the Shapley value, but it also provides the applicant some actionable information about their outcome, e.g.,  “Having a high income and number of consecutive days of employment really helped my loan application, I should take this into account when I’m thinking about retiring.” In a way, this makes a SHAP explanation analagous to a : it informs you of any potential changes you might be able to make to influence your outcome.  MODEL BACKGROUND CHOICE Since the explanations produced by SHAP are comparisons against the background dataset, the choice of background data is highly influential on the returned Shapley values. For example, here are three sets of SHAP explanations for the same model and input (the specific details of which aren’t important), just with three different choices of background: Despite the input data point and model not changing, the directions of the Shapley values vary wildly; for example, the first Shapley value is +13 for the first background selection, +0.27 for the second, and -6.74 with the third. All of these these Shapley values arrive at the same end prediction, and thus are all accurate explanations of the model’s behavior, but convey different information due to their differing comparisons. The choice as to which set of values is better is subjective and domain specific, and as such  the choice of background datapoints needs to done with careful intent. In general, you should try playing around with various choices to see what produces the clearest and most helpful explanations.   CONCLUSIONS In this post, we’ve explored the theoretical foundation of SHAP, examined how the SHAP Kernel Explainer produces explanations, and how we can interpret these explanations as much insight as we can. To see a video overview of SHAP with some different examples, check out SHAP will be joining LIME and Counterfactuals in TrustyAI’s XAI suite shortly, so stay tuned for more updates!    The post appeared first on .</content><dc:creator>Rob Geada</dc:creator></entry><entry><title>Automate dependency analytics with GitHub Actions</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/30/automate-dependency-analytics-github-actions" /><author><name>Divyanshu Agrawal</name></author><id>deb68164-3913-4242-85eb-1588fd2011f1</id><updated>2021-11-30T07:00:00Z</updated><published>2021-11-30T07:00:00Z</published><summary type="html">&lt;p&gt;Security flaws in third-party libraries are so common that many organizations integrate automated dependency analysis into their development workflows. This article introduces a new &lt;a href="https://docs.github.com/en/actions"&gt;GitHub Action&lt;/a&gt; for &lt;a href="https://github.com/redhat-actions/crda"&gt;Red Hat CodeReady Dependency Analytics&lt;/a&gt; that lets you integrate code scanning into your &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration&lt;/a&gt; (CI) pipeline while you are developing your code in a GitHub repository.&lt;/p&gt; &lt;h2&gt;Automating security in the CI/CD pipeline&lt;/h2&gt; &lt;p&gt;Developers always hope that the dependencies that are present or added to a project's manifest are free of vulnerabilities. Over the past few years, software code security has gone from a "nice to have" addition to a necessity. Many organizations today are taking steps to ensure that code going into production does not pose any security risks.&lt;/p&gt; &lt;p&gt;A common solution for this problem is to integrate a code scanning tool into the &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration (CI)&lt;/a&gt; pipeline that checks for vulnerabilities in the incoming dependencies or performs nightly checks to scan for newly introduced security risks. Code scanning lets you detect vulnerabilities prior to release, eliminating the cybersecurity risks that they involve. A number of databases, notably the &lt;a href="https://cve.mitre.org"&gt;CVE list&lt;/a&gt;, provide interfaces to tools that check the databases' contents against dependencies in code.&lt;/p&gt; &lt;p&gt;Integrating a security analysis tool with the CI workflow augments the developers' ability to add security validation while updating incoming dependencies. The addition of the Red Hat CodeReady Dependency Analytics GitHub Action to the GitHub marketplace has made code scanning easier for users in the CI/CD process.&lt;/p&gt; &lt;h2&gt;What is CodeReady Dependency Analytics?&lt;/h2&gt; &lt;p&gt;CodeReady Dependency Analytics, introduced in the article &lt;a href="https://developers.redhat.com/blog/2020/08/28/vulnerability-analysis-with-red-hat-codeready-dependency-analytics-and-snyk"&gt;Vulnerability analysis with Red Hat CodeReady Dependency Analytics and Snyk Intel&lt;/a&gt;, provides vulnerability and compliance analysis for your application's dependencies, along with recommendations to address security vulnerabilities and licensing issues. The &lt;a href="https://snyk.io/product/vulnerability-database/"&gt;Snyk Intel Vulnerability Database&lt;/a&gt; is a curated database of both new and previously known open source software security advisories.&lt;/p&gt; &lt;p&gt;CodeReady Dependency Analytics is also available as a plug-in for &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.fabric8-analytics"&gt;Visual Studio Code&lt;/a&gt;, &lt;a href="https://www.eclipse.org/che/"&gt;Eclipse Che&lt;/a&gt;, &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;Red Hat CodeReady Workspaces,&lt;/a&gt; and &lt;a href="https://www.jetbrains.com/products/"&gt;JetBrains IntelliJ-based IDEs&lt;/a&gt;. These integrations allow developers to "shift left" to detect and fix potential security vulnerabilities in the development process. Using the analytics in an IDE helps fix the security issues early, but adding the same tool into your continuous integration pipeline ensures that you don’t miss any security issues before the code goes into production.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: See &lt;a href="https://developers.redhat.com/blog/2021/04/15/vulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics"&gt;Vulnerability analysis for Golang applications with Red Hat CodeReady Dependency Analytics&lt;/a&gt; for an introduction to using CodeReady Dependency Analytics with applications written in Go.&lt;/p&gt; &lt;h2&gt;A GitHub Action for vulnerability analysis&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://github.com/redhat-actions/crda"&gt;CodeReady Dependency Analytics Github Action&lt;/a&gt; is a vulnerability scanner that uses CodeReady Dependency Analytics in the GitHub Actions space. This action scans for vulnerabilities in the project's dependencies and uploads results to the GitHub repository as a &lt;a href="https://github.com/microsoft/sarif-tutorials/blob/main/docs/1-Introduction.md"&gt;SARIF&lt;/a&gt; file, which allows vulnerabilities that are found to be integrated into the project's &lt;strong&gt;Security&lt;/strong&gt; tab. To date, this action supports applications written in Go, Python, Node.js, and Java.&lt;/p&gt; &lt;p&gt;This action also works seamlessly with pull requests, so project owners can check for vulnerabilities in the project dependencies when they are modified by a pull request.&lt;/p&gt; &lt;h2&gt;Setting up the GitHub Action&lt;/h2&gt; &lt;p&gt;The steps described in this article are the minimal steps required to run a CodeReady Dependency Analytics scan on any project. However, you may need to perform additional steps depending on your project requirements.&lt;/p&gt; &lt;h3&gt;Install the command-line interface&lt;/h3&gt; &lt;p&gt;The first thing to configure before running the action is the CodeReady Dependency Analytics command-line interface (CLI). This can be easily installed using the &lt;a href="https://github.com/redhat-actions/openshift-tools-installer"&gt;OpenShift Tools Installer GitHub Action&lt;/a&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: Install CRDA cli uses: redhat-actions/openshift-tools-installer@v1 with: source: github crda: latest &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Set up the tool stack&lt;/h3&gt; &lt;p&gt;If not already done, you must set up the tool stack based on your project. For example, if you are running a Java project, you can use the &lt;a href="https://github.com/actions/setup-java"&gt;setup-java action&lt;/a&gt; to set up Java in the workflow.&lt;/p&gt; &lt;h3&gt;Configure authentication&lt;/h3&gt; &lt;p&gt;&lt;a href="https://app.snyk.io/login?utm_campaign=Code-Ready-Analytics-2020&amp;utm_source=code_ready&amp;code_ready=FF1B53D9-57BE-4613-96D7-1D06066C38C9"&gt;Sign up for the Snyk token&lt;/a&gt;, then click through the wizard. You do not need to provide it with any permissions if you don't want to. Go to &lt;strong&gt;Account settings&lt;/strong&gt; to find your Synk token (also known as the &lt;em&gt;key&lt;/em&gt;). Set this token into a &lt;a href="https://docs.github.com/en/actions/reference/encrypted-secrets"&gt;repository secret&lt;/a&gt; and provide the created secret in the &lt;code&gt;synk_token&lt;/code&gt; input.&lt;/p&gt; &lt;p&gt;The workflow looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;name: CRDA scan Java project on: push: pull_request: jobs: scan: name: Analyse Java project runs-on: ubuntu-latest steps: - name: Checkout project uses: actions/checkout@v2 - name: Install CRDA cli uses: redhat-actions/openshift-tools-installer@v1 with: source: github crda: latest - name: Setup Java uses: actions/setup-java@v2 with: distribution: temurin java-version: 11 cache: maven - name: CRDA Scan and upload SARIF id: scan uses: redhat-actions/crda@v1 with: snyk_token: ${{ secrets.SNYK_TOKEN }} - name: Print Report Link run: echo ${{ steps.scan.outputs.report_link }}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the workflow finishes successfully, the &lt;strong&gt;Security&lt;/strong&gt; tab in the GitHub user interface (UI) is populated with the vulnerabilities found and has the relevant details. Figure 1 shows the details of a particular vulnerability.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image1_1.png?itok=_B9HY-vj" width="1186" height="758" alt="The Security tab in the GitHub interface shows details of vulnerabilities found." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The Security tab in the GitHub interface shows details of vulnerabilities found. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Additional features in the GitHub UI&lt;/h2&gt; &lt;p&gt;Before we finish, let's look at some key features of CodeReady Dependency Analytics that work in the GitHub user interface instead of the CodeReady Dependency Analytics CLI.&lt;/p&gt; &lt;h3&gt;SARIF output and upload&lt;/h3&gt; &lt;p&gt;To support a one-stop solution for dependency scanning and uploading, this GitHub Action supports SARIF output and upload. The action uploads the SARIF file to GitHub, which then displays the vulnerabilities details in the GitHub &lt;strong&gt;Security&lt;/strong&gt; tab as well as inline within pull requests.&lt;/p&gt; &lt;p&gt;Figure 2 shows the vulnerability details in the GitHub UI for a pull request when the code scan finishes.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image2_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image2_1.png?itok=s4v_jRfo" width="1123" height="797" alt=" Each pull request is enhanced with detailed notifications of vulnerabilities found." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Each pull request is enhanced with detailed notifications of vulnerabilities found. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Code scanning in pull requests&lt;/h3&gt; &lt;p&gt;Many organizations devote time to code reviews, and having another set of eyes on the modified code is always good. Organizations have also started to realize the value of reviewing security issues in the code, but not everyone has the expertise to look for them. Therefore, adding code scanning to the pull request checks substantially aids the reviews of pull requests.&lt;/p&gt; &lt;p&gt;The CodeReady Dependency Analytics GitHub Action can run code scans on pull requests. To scan pull requests, the pull request's code must be checked out. Therefore, to avoid running malicious code, the action comes with a preconfigured mechanism for labeling pull requests that allows maintainers to verify a pull request's code before installing any dependencies or running scans. Once the CodeReady Dependency Analytics scan is complete, the pull request is labeled according to the scan’s result, as shown in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image3_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image3_0.png?itok=Gw7gD-HL" width="941" height="514" alt="A scan that runs on each pull request adds a label to it." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. A scan that runs on each pull request adds a label to it. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Manifest autodetection&lt;/h3&gt; &lt;p&gt;The CodeReady Dependency Analytics GitHub Action makes it easier to scan a project’s dependencies by providing a built-in manifest detection feature. If the project uses a standard default manifest name, the user doesn’t have to explicitly provide it. However, the manifest name can be provided in the input &lt;code&gt;manifest_file&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Dependency installation&lt;/h3&gt; &lt;p&gt;The CodeReady Dependency Analytics CLI expects users to install project dependencies before running a code scan. This CodeReady Dependency Analytics GitHub Action makes dependency installation easier by automatically installing dependencies. The action has a default installation command corresponding to each supported language, which generally fulfills most projects' requirements. However, you can provide a custom dependency installation command using &lt;code&gt;deps_install_cmd&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Millions of people use GitHub for software development, and most incorporate dependencies from open source libraries. The only way to keep up with newly discovered vulnerabilities and avoid inserting these security risks into your application is to integrate scanning into your routine workflows. The CodeReady Dependency Analytics GitHub Action provides invaluable protection to your code by finding and displaying vulnerabilities while you are developing your code on GitHub.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/30/automate-dependency-analytics-github-actions" title="Automate dependency analytics with GitHub Actions"&gt;Automate dependency analytics with GitHub Actions&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Divyanshu Agrawal</dc:creator><dc:date>2021-11-30T07:00:00Z</dc:date></entry><entry><title type="html">Edge Medical Diagnosis - Example architecture with GitOps</title><link rel="alternate" href="http://www.schabell.org/2021/11/edge-medical-diagnosis-with-gitops.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/11/edge-medical-diagnosis-with-gitops.html</id><updated>2021-11-30T06:00:00Z</updated><content type="html">Part 4 - Example architecture with GitOps In our  from this series we talked about the example predictive analysis architecture found in an edge medical diagnosis solution for the healthcare industry. The process was laid out how we approached the use case and how portfolio solutions are the base for researching a generic architecture. It continued by discussion how we approached the use case by researching successful customer portfolio solutions as the basis for a generic architecture. Now it's time to look at one final example architecture. This article walks you through an example architecture for using GitOps for providing a deployment and development example for edge medical diagnosis scenarios. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are based on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered while researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this architecture and outline the solution for two views of the edge medical diagnosis architecture solution. EDGE MEDICAL DIAGNOSIS WITH GITOPS After touring the solution architecture and elements in the previous article related to actual edge medical diagnosis, this diagram shows us something entirely different. It's a matter of perspective and now we are looking at it from a development and operations perspective, using the lens of GitOps. In the diagram above we start on the right side by the developer and IT operations users, who both are developing their own specific code bases. The developer is working on services, applications, and other integration aspects in their projects. Over in IT operations they are concerned with deployments and infrastructure as code for both agility and scalability, using for example configuration as code projects. Both of these users are pushing their contributions into the source code management (SCM) system where it's then pulled into the CI/CD pipeline for the build processing which could require the use of, for example, standard base images from the remote image repository (shown here as the Red Hat repository). These builds result in application and service images in the image repository for developers which is then pushed out to the subscribed diagnostic facilities in the field. For the IT operations you'll see their configuration and manifest code pushed out to the subscribed diagnostic facilities where they host source code management (SCM) systems in the field.  At this point we transition our discussion to the diagnostic facility where the field application of the medical diagnosis solutions are applied. A GitOps element can be found that uses Argo CD to manage specific resources, including images and operations code based configurations and manifests to define the management of users, deployments, and architectural behaviours.  Finally, we see this all come to fruition on the far left of this diagram as the control plane, a container platform based on OpenShift, hosts the various services and applications in their container-based runtimes. Next up, a look at the architectural solution with a focus on the data view. EDGE MEDICAL DIAGNOSIS WITH GITOPS (DATA) Data connectivity through the edge medical diagnosis architecture with GitOps provides a different look at the architecture and gives us insights into how one of the most valuable assets of a healthcare organisation is being processed. It should be seen as the architecture is intended, as a guide and not a definitive must-do-it-this-way statement on how the data is being routed through as actors are engaging with the systems, applications, and services in this architecture. Note that many of the data flows only one direction while it's fairly obvious it's going to flow both ways. We've chosen to note that in the flows that do not disrupt the clarity of the architecture diagram, and chose not to indicate where the intent is to show processing and flows for clarity from actors to systems on the backend. It's left to the reader to explore these data diagrams and feel free to send comments our way. WHAT'S NEXT This was an overview of the specific pneumonia detection schematic diagrams that make up our architecture for the edge medical diagnosis use case.  An overview of this series on edge medical diagnosis architecture: 1. 2. 3. 4. Catch up on any articles you missed by following one of the links above. This completes the series and we hope you enjoyed this edge medical diagnosis architectural tour.</content><dc:creator>Eric D. Schabell</dc:creator></entry></feed>
